{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "What you will Learn :\n",
    "\n",
    "- How do you know whether or not a data feel will be useful inside of your model?\n",
    "- What are those rules of making a feature good? And how can you create new features to further enrich your data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data to Features\n",
    "\n",
    "- Good Feature Engineering, this process that we're going to go through, can take an average of 50 to 75 percent of the time that you working on an ML project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good vs Bad Feature\n",
    "\n",
    "- Different problems in the same domain, may need different features, and it depends on you, and your subject matter expertise to determine which fields you want to start with for your hypothesis.\n",
    "\n",
    "- Good feature has to be related to the objective, you can't just throw a random data in there. You want to take your raw data and represent it in a form that's amenable to machine learning. \n",
    "\n",
    "- It's got to be numeric\n",
    "\n",
    "- You've got to have enough examples for it in your data set\n",
    "\n",
    "- You need to bring in your own human insights into the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Known at Prediction Time\n",
    "\n",
    "- The second aspect of a good feature, you need to know the value at the time that you're actually predicting\n",
    "\n",
    "- Your data warehouse has the information because somebody has already gone through the trouble of taking all the data or joining all the tables together and putting on an pre-processing in there. But at prediction time, in real-time, you don't have it. So, therefore, you can't use it\n",
    "\n",
    "- Feature definitions themselves should not change over time,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Should be Numeric\n",
    "\n",
    "- Ultimately a neural network is simply an adding, multiplying and a weighing machine\n",
    "\n",
    "- Your inputs better be numbers and those magnitudes better have some meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Should Have Enough Examples\n",
    "\n",
    "- Avoid having issues like this where you don't have enough examples, notice that I'm not saying that you have to have at least five categories, I'm saying that you have to have at least five samples\n",
    "\n",
    "- So, for every value of a particular column you need to have those five examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing Human Insight\n",
    "\n",
    "- You need to have your subject matter expertise and your own curious mind to think of all the ways the data field could be used as a feature or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing Features\n",
    "\n",
    "- You have to preprocess that data and find all the key is it occurring in a trading dataset, and create what's called a vocabulary of keys\n",
    "\n",
    "- Suppose you don't have a vocabulary, and it's not energerized, well, here's a cool trick that you can do. If you don't want to go out, build a vocabulary and you don't really care, so, what are you going to do is, I'm going to take my employee ID, hash it, compute the hash of the employee ID, and just break that hash up into say 500 buckets\n",
    "\n",
    "- You have to add an extra column to state whether or not you observed the value or not. So, if you have missing data, the long or short of it is, you need to have another column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML vs Statistics\n",
    "\n",
    "- Statistics on the other hand is about keeping the data that you have in getting the best results out of the data that you have\n",
    "\n",
    "- Statistics is often used in a limited data regime or ML operates with lots of data\n",
    "\n",
    "- In ML you go out and find enough outliers that becomes something that you can actually train with\n",
    "\n",
    "- In ML the idea is that you build the separate model for this situation where you have the data versus when you don't\n",
    "\n",
    "- What do we do with a magnitude piece? Well, what if we did this, instead of having one floating point feature let's take a look and have 11 distinct boolean features\n",
    "\n",
    "- Modern architectures for ML end up taking a variable magnitudes into account because of what's called batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve model accuracy with new features\n",
    "\n",
    "- obviously all those input features were useful. Right? Because again the loss went up, when we didn't have them. So this gives you an idea of what kind of features are useful and you could do this manually, you could basically go ahead and say, what happens if I just use the median income. And the median income is a pretty good indicator\n",
    "\n",
    "- They way this matters in the real-world is, it really matters what data you collect. Imagine that you had this dataset, and you did not collect the median income of people who lived in that neighborhood.\n",
    "\n",
    "- What matters for a machine learning model is not the model itself, but the data that put into the model\n",
    "\n",
    "- This is a very important feature. And in order to have this feature, you need to have the data engineering pipeline to bring this data in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Feature Creation\n",
    "\n",
    "- BigQuery and Apache Beam will be used to process the full input dataset prior to training. This covers operation like excluding some data points from the training dataset and also, computing summary statistics and vocabularies over the entire input dataset.\n",
    "\n",
    "- Many common preprocessing steps can be written using one of the existing methods from the TensorFlow of feature columns API. For example, if you need to change a real value feature into a discrete one, you can use the bucket dice column method. If the feature pre-processing step that you need is not available in the TensorFlow APIs, you can modify the functions used in the input parameters during training, validation, and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Beam and Cloud Dataflow\n",
    "\n",
    "- What is Cloud Dataflow? One of the ways to think about feature pre-processing, or even any data transformation, is to think in terms of pipelines\n",
    "\n",
    "- Cloud Dataflow is a platform that allows you to run these kinds of data processing pipelines. Dataflow can run pipelines written in Python and Java programming languages\n",
    "\n",
    "- One thing that makes Apache Beam easy to use is that the code written for Beam is similar to how people think of data processing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing with Cloud Dataprep\n",
    "\n",
    "- When working with a data set that you have never seen before, you should start with an exploratory analysis, you should visualize the values of the data set, understand which values happened frequently and infrequently, find outliers and look for missing values\n",
    "\n",
    "- You definitely want to know the statistics of the data set, averages, standard deviation for different variables in your data, there are minimum and maximum values\n",
    "\n",
    "- You want to explore the distributions of these values. Also, when working on machine learning, chances are you working with a team that can include data scientists, software developers, and business analysts.\n",
    "\n",
    "- The first approach, we'll use the tools that you have already seen including BigQuery, Cloud Dataflow and Tenserflow\n",
    "\n",
    "- The second approach, we'll introduce Cloud Dataprep, and show you how Dataprep can help with both exploratory analysis and data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Feature Crosses\n",
    "\n",
    "- You will then learn how to incorporate feature creation as part of the machine learning pipeline. And finally, you will put all these lessons together to improve the taxifare model using Feature Crosses\n",
    "\n",
    "### What is a Feature Cross?\n",
    "\n",
    "- feature crosses are a way to bring non-linear inputs to a linear learner, a linear model. But there is a bit of a caveat\n",
    "\n",
    "- Feature crosses also let you combine features. And the good thing is, you can get a way with the simpler model, a linear model, and this is a good thing, simpler models are a good thing.\n",
    "\n",
    "### Discretization\n",
    "\n",
    "- I needed the white lines in just the right position, before I could say that x_3 alone was enough to separate the space. The white lines here are helping to discretize the input space\n",
    "\n",
    "### Memorization vs. Generalization\n",
    "\n",
    "- You should realize that feature crosses are about memorization, and memorization is the opposite of generalization which is what machine learning aims to do\n",
    "\n",
    "- Memorization works when you have so much data that for any single grid cell in your input space, the distribution of data is statistically significant\n",
    "\n",
    "- Incidentally, if you're familiar with traditional machine learning, you may not have heard much about feature crosses. The fact that feature crosses memorize and only work on larger data sets is one reason that you may not have heard much about it\n",
    "\n",
    "- Feature crosses or a powerful pre-processing technique on large data sets\n",
    "\n",
    "### Benefit and Tradeback of Feature Crosses\n",
    "\n",
    "- Using feature crosses plus massive data is a very efficient strategy for learning highly complex spaces\n",
    "\n",
    "- feature crosses allow a linear model to memorize large datasets. The idea is, you can assign a weight to each feature cross, and this way the model learns about combinations of features\n",
    "\n",
    "- Neural networks with many layers are non-convex. But optimizing linear models is a convex problem, and convex problems are much, much, much easier than non-convex problems.\n",
    "\n",
    "- Using sparse linear models as a pre-processor for your features will often mean that your neural network converges much faster\n",
    "\n",
    "- If you have feature crosses, even though you're using a linear model, because the feature cross is non-linear, you actually have a non-linear model\n",
    "\n",
    "- If we use a model that is too complicated for such simple data, if we use a model with too many feature crosses, we give it the opportunity to fit to the noise in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Feature Crosses\n",
    "\n",
    "- To create a feature cross using TensorFlow, use the method crossed column in the model tf.feature_column\n",
    "\n",
    "- When your bucketize a numeric column, you're essentially drawing those black lines we talked about. You're discretizing the column. So, what is the second argument? 24 times 7 is the total number of hash buckets.\n",
    "\n",
    "- In practice, I tend to choose a number between half square root n and twice n depending on how much I want to trade-off memorization versus sparsity, but this is simply my rule of thumb.\n",
    "\n",
    "### Embedding Feature Crosses\n",
    "\n",
    "- Feature crosses and embeddings are very useful in real world machine learning models\n",
    "\n",
    "- Using a large value for hash buckets leads to a very sparse representation\n",
    "\n",
    "- Fine grained predictions, that's what machine learning is about.\n",
    "\n",
    "- Our predictions are going to be, number of cars, number of trucks, number of bicycles, at any given time, at any given point in the city.\n",
    "\n",
    "- The key thing is that similar day hour combinations in terms of traffic tend to be similar, and day hour combinations that have very different traffic conditions tend to be far apart in the two dimensional space. This is what we mean when we say that the model learns to embed the feature cross in a lower-dimensional space\n",
    "\n",
    "- We'll see embeddings a lot than we get the language models. There you might have 100,000 unique words and you want to embed them, represent them in the lower-dimensions space of maybe 30 or 50 dimensions\n",
    "\n",
    "### Where to Do Feature Engineering\n",
    "\n",
    "- What embedding does is it allows the grid cells that are similar to each other, maybe all the grid cells that are along the ocean front take all this grid cells, and make them have similar values.\n",
    "\n",
    "- What happens when the feature across the two discretized values. We have essentially taken the map and broken it into grid cells such that any house belongs to only one of those grid cells. So, during training this will allow us to memorize the average price of houses in each grid cell\n",
    "\n",
    "- The finer the resolution of the grid the more specific the prediction will be. But it will also be less generalizable, because there might not be enough houses sold in a grid cell for us to form a good estimate\n",
    "\n",
    "### Feature Creation in DataFlow\n",
    "\n",
    "- Dataflow is a fine place to do preprocessing. Dataflow is ideal for features that involve time-windowed aggregation.\n",
    "\n",
    "- For example, you might want to use as a feature the average number of people who look at a product in the past one hour\n",
    "\n",
    "- This is very advantageous because dataflow is great at computing aggregates over all of the data. While TensorFlow is advantageous when it comes to manipulating the input fields on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing TensorFlow Transform\n",
    "\n",
    "- Using tf.transform will allow us to carry out feature processing efficiently, at scale and on streaming data.\n",
    "\n",
    "### TensorFlow Transform\n",
    "\n",
    "- There are two PTransforms in tf.transform. AnalyzeAndTransformDataset, which is executed in Beam to create a preprocessed training dataset, and TransformDataset which is executed in Beam to create the evaluation dataset\n",
    "\n",
    "- Computing the min and max, et cetera the analysis is done only on the training dataset\n",
    "\n",
    "- We cannot use the evaluation dataset for that. So, the evaluation dataset is scaled using the min and max found in the training data\n",
    "\n",
    "- There are two phases. The analysis phase. This is executed in Beam while creating the training dataset. The transform phase. This is executed in TensorFlow during prediction. So, executed in Beam to create your training and evaluation datasets.\n",
    "\n",
    "### Analyze and Transform phase\n",
    "\n",
    "- The analyze and transform P transform happens on the training dataset\n",
    "\n",
    "- For the evaluation dataset, we carry a pretty much the same beam pipeline that we did on the training dataset. There's one big exception though, we don't analyze the evaluation dataset\n",
    "\n",
    "- On the evaluation dataset, we don't call analyze and transform, we just call transform data set\n",
    "\n",
    "### Supporting serving\n",
    "\n",
    "- For what type of data did we use analyze and transform dataset? Right, the training data. And we use transform dataset for the evaluation data\n",
    "\n",
    "- The DNN regressor or whatever model we use cannot deal with a string at DHU. The reason it works is that all the code that you wrote in preprocessed, that code is now part of the model graph itself. This happens because a model reads the metadata and it includes a preprocessing code\n",
    "\n",
    "### "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}