{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launching into Maching Learning\n",
    "\n",
    "- Good machine learning requires that we create datasets and models that permit generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you will learn\n",
    "\n",
    "- You learn how to optimize models using loss functions\n",
    "\n",
    "- You learn how to evaluate those models using performance metrics\n",
    "\n",
    "- You learn how model training and evaluation work, you will also learn about the common problems that can happen when you do machine learning\n",
    "\n",
    "- You will learn how to mitigate, that is, how to reduce the incidence of those kinds of problems\n",
    "\n",
    "- You will learn why you often need three identically distributed datasets, and how to create them in a repeatable way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "- Two of the most common classes of machine learning models are supervised and unsupervised ML models\n",
    "\n",
    "- In Supervised models, we have labels or in other words, the correct answers to whatever it is that we want to learn to predict. In Unsupervised learning, the data does not have labels\n",
    "\n",
    "- Within supervise ML, there are two types of problems: regression and classification\n",
    "\n",
    "- Unsupervised problems are all about discovery, about looking at the raw data and seeing if it naturally falls into groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression and Classification\n",
    "\n",
    "- Regression - Continous\n",
    "\n",
    "- Classification - Categorization\n",
    "\n",
    "- In regression problems, we want to minimize the error between our predicted continuous value, and the label's continuous value, usually using mean squared error.\n",
    "\n",
    "- In regression problems, the goal is to use mathematical functions of different combinations of features, to predict the continuous value of our label\n",
    "\n",
    "- In classification problems, instead of trying to predict a continuous variable, we are trying to create a decision boundary that separates the different classes\n",
    "\n",
    "- In a classification problem. In general, a raw continuous feature can be discretize into a categorical feature\n",
    "\n",
    "- Regression models usually use mean squared error as their loss function, whereas Cassification models tend to use cross entropy\n",
    "\n",
    "- Regression model, there is a quadratic penalty for mean squared error, so it is essentially trying to minimize the euclidean distance between the actual label and the predicted label\n",
    "\n",
    "- Classification's cross-entropy, the penalty is almost linear and the predicted probability is close to the actual label, but as it gets further away it becomes exponential, when it gets close to the predicting the opposite class of the label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History of ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History of Linear Regression\n",
    "\n",
    "- Regression problems, the loss function is mean squared error\n",
    "\n",
    "- Linear regression was invented for predicting the movement of planets, and the size of pea pods based on their appearance\n",
    "\n",
    "- Sir Francis Galton, pioneered the use of statistical methods to measurements of natural phenomena. He was looking at data on the relative sizes of the parents and children, in various species including sweet peas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percepton\n",
    "\n",
    "- In 1940s, a researcher, Frank Rosenblatt, comes up with a perceptron as a computational model of a neuron in the human brain and shows how it can learn simple functions\n",
    "\n",
    "- A single layer perceptron islike the neuron. It has inputs which it then multiplies by weights and sums all together.\n",
    "\n",
    "- The Sum value here is now compared with the threshold and then transformed by an activation function. For instance, if the sum is greater than or equal to zero, then activate or press a value of one, otherwise, don't activate or press a value of zero\n",
    "\n",
    "- We would call today, a Binary Linear Classifier where we are trying to find a single line that splits the data into two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "- Neural Networks combine layers of perceptons.\n",
    "\n",
    "- But it hard to train effectively\n",
    "\n",
    "- Without using nonlinear activation functions, all of the additional layers can be compressed back down into just a single linear layer, and there's no real benefit. \n",
    "\n",
    "- You need nonlinear activation functions like sigmoid, or hyperbolic tangent, ELU, ReLU, or tanh. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "- Decision trees are one of the most intuitive machine learning algorithms. They can be used for both classification and regression.\n",
    "\n",
    "- The first thing you should do is determine the split metric\n",
    "\n",
    "- For regression trees, mean squared error is a common metric split.\n",
    "\n",
    "- Each split is essentially just a binary linear classifier that finds a hyper plane that slices along one feature's dimension at some value, which is the chosen threshold to minimize the members of the class falling in the other classes' side of the hyperplane\n",
    "\n",
    "- In a decision classification tree, at each node in the tree, the algorithm chooses a feature and threshold pair to split the data into two subsets, and continues this recursively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Methods\n",
    "\n",
    "- Kernelized SVMs tend to provide sparser solutions and thus have better scalability. SVMs perform better when there is a high number of dimensions and when the predictors nearly certainly predict the response\n",
    "\n",
    "- There are many types of kernels with the most basic being the basic linear kernel, the polynomial kernel, and the Gaussian radial basis function kernel\n",
    "\n",
    "- If data is not linearly separable into the two classes. We can apply a kernel transformation which maps the data from our input vector space to a vector space that now has features that can be linearly separated as shown in the diagram\n",
    "\n",
    "- The number of neurons per layer that determine how many dimensions of vector space you are in. If you have two inputs and you have three neurons, you are mapping the input 2D space to a 3D space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "- The algorithm that performs this learning is an ensemble method. One of the most popular types of ensemble learning is the random forest\n",
    "\n",
    "- This group of predictors is an ensemble which when combined in this way, it leads to ensemble learning.\n",
    "\n",
    "- When you aggregate the results of many predictors either classifiers or regressors, the group will usually perform better than the best individual model\n",
    "\n",
    "- You could have a group of decision trees that each get a random subsample of the training data, and combine it together.\n",
    "\n",
    "- If this is classification, there could be a majority vote across all trees which would then be the final output class. \n",
    "\n",
    "- If it is regression, it could be an aggregate of the values such as the mean, max, median, et cetera.\n",
    "\n",
    "- To improve generalization, you can random sample the examples and/or the features\n",
    "\n",
    "- Each individual predictor has higher bias being trained on the smaller subset rather than the full dataset, but the aggregation reduces both the bias and variance. This usually gives the ensemble a similar bias as a single predictor on the entire training set, but with a lower variance\n",
    "\n",
    "- For boosted trees, which is when we aggregate a number of weak learners to create a strong learner or we add more trees to the ensemble, the predictions usually improve\n",
    "\n",
    "- So, do we continue to add trees ad infinitum? Of course not. You can use your validation set to use early stopping, so that we don't start overfitting our training data, because we've added too many trees\n",
    "\n",
    "- Similar to neural networks, the more layers and complexity you add to your random forest, the more difficult it will be to understand and explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modern Neural Networks (Deep Learning)\n",
    "\n",
    "- Modern Neural Networks, include all of other ML technique advantage\n",
    "\n",
    "- The use of nonlinear activation functions such as ReLUs, which usually are set as the default in Modern NN\n",
    "\n",
    "- Dropout layers began being used to help with generalization, which works like ensemble methods, which we explored when talking about random forests and boosted trees\n",
    "\n",
    "- Convolutional layers were added that reduced the computational and memory load due to their non-complete connectedness, as well as being able to focus on local aspects, for instance, images, rather than comparing unrelated things in an image\n",
    "\n",
    "- Machine learning is all about experimentation. There are so many different types of algorithms, hyperparameters and ways to create your machine learning datasets these days\n",
    "\n",
    "- You need to make sure you have lots of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "What you will learn\n",
    "\n",
    "- How to measure model performance objectively using loss functions\n",
    "\n",
    "- Use loss functions as the basis for an algorithm called gradient descent\n",
    "\n",
    "- Optimize gradient descent to be as efficient as possible\n",
    "\n",
    "- Use performance metrics to make business decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining ML Model\n",
    "\n",
    "- ML models are mathematical functions with parameters and hyper-parameters. A parameter is a real-valued variable that changes during model training.\n",
    "\n",
    "- A hyper-parameter is a setting that we set before training and which doesn't change afterwards.\n",
    "\n",
    "- Modelling a non-linear function with a linear model is an example of what's called underfitting.\n",
    "\n",
    "- when an analytical solution is no longer an option, You use gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducingg to Loss Function\n",
    "\n",
    "- The functions that generalize from the error of a prediction, to a number that captures the quality of a group of predictions, are known as loss functions.\n",
    "\n",
    "- One of the most commonly used loss functions for regression is RMSE\n",
    "\n",
    "- RMSE is the root of the mean squared error. The bigger the RMSE, the worse the quality of the predictions. So what we want to do is, to minimize RMSE\n",
    "\n",
    "- One of the most commonly used loss functions for classification is called Cross Entropy, or log loss.\n",
    "\n",
    "- Cross entropy penalizes bad predictions very strongly, even in this limited domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "- Gradient Descent refers to the process of walking down the surface formed by using our loss function on all the points in parameter space\n",
    "\n",
    "- A common scenario in machine learning is we waste days of time to train our model\n",
    "\n",
    "- We need a scaling parameter to make the training step adaptive so it can become faster. In the literature, this is referred to as the learning rate\n",
    "\n",
    "- Learning rate is a hyperparameter and likely to have a problem-specific best value. Generally though, learning rate is a fraction significantly less than one\n",
    "\n",
    "- to determine the best value for hyperparameters, there is a better method available and it's called hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model Pitfall\n",
    "\n",
    "- For many models, if you retrain the model a second time, even when using the same hyperparameter settings, the resulting parameter settings might be very different. The formal name for this property is convexity.\n",
    "\n",
    "#### How can we make the training fast?\n",
    "\n",
    "- We can sampling the number of data points that we  use to calculate the derivative. We refer to this practice of sampling from our training set during training as mini-batching. Batch sizes is another hyperparameter, but typically, batch sizes between 10 and 1,000 examples\n",
    "\n",
    "- We can configure the frequency with which we check the loss. Some popular strategies for the ready-to-update loss function are time-based and step-based. For example, once every 1,000 steps or once every 30 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Rule of Thumbs in ML\n",
    "\n",
    "- As the learning rate increased, the loss curve steepened\n",
    "\n",
    "- when the learning rate was 10, the first step changed the weights dramatically\n",
    "\n",
    "- The magnitude of the weights increased as the learning rates increase. This is because the model is taking bigger steps\n",
    "\n",
    "- The decision boundary does a poor job of dividing the data by class. The reason is that the data have a non-linear relationship,\n",
    "\n",
    "- As batch size increased, so did the smoothness\n",
    "\n",
    "- Changes in batch size will have a simple effect on the rate of convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Curve Troubleshooting\n",
    "\n",
    "- Advanced optimization techniques aim to improve training time and help models not to be seduced by local minima.\n",
    "\n",
    "- Data weighting and oversampling and synthetic data creation aim to remove inappropriate minima from the search space altogether.\n",
    "\n",
    "- Performance metrics change the way we think about the results of our search by aligning them more closely with what we actually care about\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics\n",
    "\n",
    "- Performance metrics have two benefits over loss functions\n",
    "\n",
    "    - Firstly, they're easier to understand. This is because they're often simple combinations of countable statistics.\n",
    "\n",
    "    - Secondly, performance metrics are directly connected to business goals\n",
    "\n",
    "- Three performance metrics: confusion matrices, precision, and recall,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "- How well our model did with its positive predictions, we use a metric called precision.\n",
    "\n",
    "- Precision is formally defined, as the number of true positives divided by the total number classified as positive.\n",
    "\n",
    "- Recall is often inversely related to precision. Recall is like a person who never wants to be left out of a positive decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization and Sampling\n",
    "\n",
    "### Introduction\n",
    "\n",
    "- A model has a loss metric of zero for your training dataset does not mean that it'll perform well on new data out there in the real world.\n",
    "\n",
    "- The best ML model is not necessarily the one that performs best on just your training dataset, but it's the one that performs best on unseen data\n",
    "\n",
    "- How well would your model perform on unknown data? Well, first you need to get some data that’s not shown to the model during training. And after you successfully train the model, you can then evaluate it on this held-out dataset\n",
    "\n",
    "- You can then split into separate training and evaluation datasets. You can then experiment and train your model with one datase\n",
    "\n",
    "- You learn actually how to create these repeatable training, evaluation and test datasets and actually establish performance benchmarks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization and ML model\n",
    "\n",
    "- For regression problems, the loss metric that you want to optimize for, is typically mean squared error, MSE or RMSE, the root mean squared error\n",
    "\n",
    "- Mean squared error tells us how close a regression line is to the set of points from it. It does this by taking those distances from the points to the actual regression line. And those distances are called the errors and then it squares them\n",
    "\n",
    "- Taking the square root of the MSE gives us the RMSE which is simply the distance on average of a data point from the fitted line measured along a vertical line. A lower value indicates a better performing model, and the closer the error is to zero the better\n",
    "\n",
    "- How can I make sure that my model is not overfitting? How do I know when to stop training? And the answer is surprisingly simple, we are going to split your data\n",
    "\n",
    "- By dividing your original dataset into completely separated and isolated groups, you can iteratively train your model and train it on the training dataset and then once you're done with training, compare its performance against an independent siloed validation dataset. And models that generalize well, will have similar loss metrics or error values across training and validation\n",
    "\n",
    "- Validation dataset to help us know when to stop training and to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Stop Training Your Data\n",
    "\n",
    "- When you start to see that divergence and you confirm that the model is not overfitting, that's when we know we need to stop and say, our model is tuned, ready for production.\n",
    "\n",
    "- You actually have to split your data into three parts, training, validation, and a brand new completely isolated silo called test or testing.\n",
    "\n",
    "- That's the loss metric that you can report to your boss. And it's the loss metric that then on your testing dataset, decides whether or not you want to use this model in production\n",
    "\n",
    "- What happens if you fail on your testing dataset, even though you passed validation? Means you can't retest the same ML model, and you've got to either retrain a brand new Machine Learning model, or go back to the drawing board and collect more data samples to provide new data for your ML model.\n",
    "\n",
    "- If you have lots of data, use the approach of having a completely independent held-out test dataset, that's like go or no-go decision. If you don't have that much data, use the cross-validation approach.\n",
    "\n",
    "- The Cross-validation is the compromise between these methods, is to do a training validation split and do that many different times. Train and then compute the loss in the validation dataset, keeping in mind this validation set could consist of points that were not used in training the first time, then split the data again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "- The dataset might also be sorted, which could add bias into your sample. And simply adding an order by also comes with its own problems when doing something like mini-batch gradient descent\n",
    "\n",
    "- Hash is used to make unique identification of parameter\n",
    "\n",
    "- If you split your data by arrival airport, set 80 percent of the airports are in the training data set and the others are in test and validation, then you'd use the hash function on arrival airport instead\n",
    "\n",
    "- What if we bucketize or hash and split on date? Okay, that's fine. But understand that you can no longer make predictions based on something like the holidays, like Christmas or Thanksgiving. Be sure the primary drivers in your prediction have nothing to do with date, because that's how you bucketed and created those buckets\n",
    "\n",
    "- Starting out with ML Model development, is best to develop your Tensorflow code on a small subset of data. Then later scale it out to the Cloud for true productionalization\n",
    "\n",
    "- If you use the full data set, this honestly could take hours or even days. You're talking petabytes of data and you can't develop software that way\n",
    "\n"
   ]
  }
 ]
}