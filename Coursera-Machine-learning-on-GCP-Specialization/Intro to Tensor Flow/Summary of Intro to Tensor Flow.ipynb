{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This note is created usingtensorflow 2.0.0\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Intro to Tensor Flow\n",
    "\n",
    "# Week 1\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- You will learn how to use the TensorFlow libraries to solve numerical problems\n",
    "\n",
    "- You will learn how to troubleshoot and debug common TensorFlow called pitfalls\n",
    "\n",
    "- You will learn how to debug TensorFlow programs\n",
    "\n",
    "- Understanding how well a machine learning model is doing will require you to views KLR numbers like losses and weights over the course of training as a chart\n",
    "\n",
    "- You will learn what I mean when I say lazy evaluation imperative, and learn how to write lazy evaluation and imperative programs\n",
    "\n",
    "- Lazy evaluation means that TensorFlow works of variables that are parts of graphs that are tied to sessions\n",
    "\n",
    "- You also typically want to look at things called embeddings, or projectors, and the architecture of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Tensor Flow?\n",
    "\n",
    "- Vector to mean 1D arrays. A two-dimensional array is a matrix, but the three-dimensional array, we just call it a 3D tensor. So scalar, vector, matrix 3D tensor, 4D tensor et cetera.\n",
    "\n",
    "- A tensor is an n dimensional array of data. So your data in TensorFlow, they are tensors\n",
    "\n",
    "- The way TensorFlow works is that you create a directed acyclic graph, a DAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of directed acyclic graph, a DAG\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/Topological_Ordering.svg/1200px-Topological_Ordering.svg.png\" alt=\"DAG\" style=\"width: 250px;\"/>\n",
    "\n",
    "- Portability, the directed acyclic graph, the DAG is a language-independent representation of the code in your model\n",
    "\n",
    "- You can use the same Python code and execute it on both CPUs and GPUs, so it gives you language and hardware portability\n",
    "\n",
    "- Like JVM in Java, Python code will execute by Tensor Flow Engine\n",
    "\n",
    "- You can train a TensorFlow model on the cloud, on lots and lots of powerful hardware, and then take that trained model and put it on mobile device.\n",
    "\n",
    "- Like Google Translate App that can work completely offline because a trained translation model is stored on the phone and is available for offline translation.\n",
    "\n",
    "- These sorts of smaller, less powerful models are typically implemented using TensorFlow Lite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training on Mobile Device\n",
    "\n",
    "- One situation is that you train a model, and then you deploy to a bunch of phones. And then when you make a prediction, the user says nope, this isn't right, or please show me more results like this\n",
    "\n",
    "- You want to update the weights of the model to reflect that user's preferences. Which, you can set up what is called federated learning, where you aggregate many users' updates.\n",
    "\n",
    "- This aggregate is essentially like a weight update on a batch of samples, except that it comes from different users\n",
    "\n",
    "- this consensus change happens to the shared model on the cloud. So you deploy the shared model, you fine tune it on different users' devices, rinse and repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Flow Hierarchy\n",
    "\n",
    "- You don't need a custom neural network model, many times you are quite happy to go with a relatively standard way of training\n",
    "\n",
    "- You don't need to customize the way you train, you're going to use one of a family of gradient descent optimizer, and you're going to back propagator the weights, and you're going to do this iteratively. In that case, don't write a low level session loop. Just use an estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Flow Hierarchy API\n",
    "\n",
    "![tensorflow_herarchy](https://developers.google.com/machine-learning/crash-course/images/TFHierarchy.svg?hl=id)\n",
    "\n",
    "- TensorFlow has in it a number of abstraction layers\n",
    "\n",
    "- The lowest level of abstruction, is a layer that's implemented to target different hardware platforms\n",
    "\n",
    "- The next level, is a TensorFlow c++ API. This is how you can write a custom TensorFlow app, You will implement a function you want in C++, and register it as a TensorFlow operation\n",
    "\n",
    "- The core Python API the next level, is what contains much of the numeric processing code, add, subtract, divide, matrix multiply etc. creating variables, creating tensors, getting the shape, all the dimensions of a tensor, all that core basic numeric processing stuff, that's all in the python API\n",
    "\n",
    "- Then the next one is in tf layers, a way to compute the root mean square error and data as it comes in, tf metrics, a way to compute cross entropy with Logic's. This is a common last measurement classification problems, cross entropy with logits, it's in tf losses.\n",
    "\n",
    "- The estimator, is the high-level API in TensorFlow. It knows how to do this to be the training, it knows how to evaluate how to create a checkpoint, how to Save a model, how to set it up for serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph and Session\n",
    "\n",
    "### Benefit of using directed graph\n",
    "\n",
    "- Tensorflow can assign different parts of the DAG to different devices, depending on whether it's I/O bound, or whether it's going to require GPU capabilities\n",
    "\n",
    "- Tensorboard is used to visualize the graph\n",
    "\n",
    "- When the graph is being compiled, TensorFlow can take two ops and fuse them to improve performance, e.g. if there are 2 add operation it will be fuse into 1\n",
    "\n",
    "- The most exciting part is that the DAG can be remotely executed and assigned to devices. It's possible for TensorFLow to partition your program across multiple devices; CPUs, GPUs, TPUs, etc that are attached even to different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy Evaluation Mode\n",
    "\n",
    "- Why does TensorFlow do lazy evaluation? It's because lazy evaluation allows for a lot of flexibility and optimization when you're running the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3f786b9aec9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# to evaluate tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "# to evaluate tensor\n",
    "sess.run(z)\n",
    "z.eval()\n",
    "sess.run([z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tensorflow 1.x, we still know Session, but in Tensorflow 2.X, we dont need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'PrintV2' type=PrintV2>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# build the DAG\n",
    "a = tf.constant([3,5,7])\n",
    "b = tf.constant([1,2,3])\n",
    "\n",
    "c = tf.add(a,b)\n",
    "\n",
    "# run it\n",
    "tf.print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'PrintV2_1' type=PrintV2>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.executing_eagerly()\n",
    "\n",
    "x = tf.constant([3,5,7])\n",
    "y = tf.constant([1,2,3])\n",
    "\n",
    "tf.print(x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor and Variable\n",
    "\n",
    "- We can do the stacking in code instead, instead of counting all those parenthesis.\n",
    "\n",
    "- You can also slice a tensor to pull out lower dimensional tensors\n",
    "\n",
    "- Once you have the data into a tensor, you can take all that data and it can reshape the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacking\n",
      "slicing\n",
      "reshaping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'PrintV2_6' type=PrintV2>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# stacking\n",
    "print('stacking')\n",
    "a = tf.constant([3,5,7])\n",
    "b = tf.stack([a,a])\n",
    "tf.print(b)\n",
    "\n",
    "# slicing\n",
    "print('slicing')\n",
    "tf.print(b[:,1])\n",
    "tf.print(b[1,:])\n",
    "tf.print(b[1,0:2])\n",
    "\n",
    "# reshape tensor\n",
    "print('reshaping')\n",
    "tf.print(tf.reshape(b, [3,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable vs Place holder\n",
    "\n",
    "- We initialize variables, we can modify it later as well. Whereas placeholder doesnâ€™t require initial value. Placeholder simply allocates block of memory for future use. \n",
    "\n",
    "- Later, we can use feed_dict to feed the data into placeholder\n",
    "\n",
    "Using tensorflow 1, to use placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_2:0\", dtype=float32)\n",
      "[ 4.  8. 12.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "a = tf.placeholder(\"float\", None)\n",
    "b = a * 4\n",
    "print(a)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    print(session.run(b, feed_dict={a: [1,2,3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Excercise : Halley Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.29887062, 0.00010644918, 0.001)\n",
      "-0.29887062\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "def compute_halley_method(x, ratio, tol):\n",
    "  # slice the input to get the coefficient\n",
    "\n",
    "    coefficient = tf.constant(\n",
    "      [5.0, 3.0, 7.0, 8.0, 9.0]\n",
    "    )\n",
    "    \n",
    "    a0 = coefficient[0]\n",
    "    a1 = coefficient[1]\n",
    "    a2 = coefficient[2]\n",
    "    a3 = coefficient[3]\n",
    "    a4 = coefficient[4]\n",
    "    \n",
    "    ddf_x = 2 * a2 + 6 * a3 * x + 12 * a4 * x**2\n",
    "    df_x = a1 + 2 * a2 * x + 3 * a3 * x**2 + 4 * a4 * x**3\n",
    "    f_x = a0 + a1 * x + a2 * x**2 + a3 * x**3 + a4 * x**4\n",
    "  \n",
    "    # Halley's formula\n",
    "    num = (2 * f_x * df_x)\n",
    "    den = tf.subtract((2 * df_x * df_x), (f_x * ddf_x))\n",
    "    ratio = tf.divide(num, den) \n",
    "    x = tf.subtract(x, ratio)\n",
    "    x = tf.Print(x, [x], 'x=')\n",
    "    return x, ratio, tol\n",
    "\n",
    "def cond(x, ratio, tol):\n",
    "    diff = tf.abs(ratio)\n",
    "    return tf.less(tol, diff)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # initial guess\n",
    "    x = 1.0\n",
    "    \n",
    "    # initial delta/ratio\n",
    "    ratio = 3.0\n",
    "    \n",
    "    # error\n",
    "    tol = 0.001\n",
    "    \n",
    "    halley_result = tf.while_loop(cond, compute_halley_method, [x, ratio, tol])\n",
    "    # 1. Full 3 parameters\n",
    "    result = sess.run(halley_result)\n",
    "    print(result)\n",
    "\n",
    "    # 2. One Parameter\n",
    "    print(halley_result[0].eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Tensor Flow Program\n",
    "\n",
    "- Debugging a TensorFlow program can be tricky because of the lazy evaluation paradigm.\n",
    "\n",
    "- This is one of the reasons why we said TF eager can be helpful when developing TensorFlow programs\n",
    "\n",
    "- If there is shape error, shape coercing can be done with reshape method\n",
    "\n",
    "- Another common error that you will run into when developing TensorFlow programs are data type errors\n",
    "\n",
    "### Debugging Whole Program\n",
    "\n",
    "- TensorFlow also has a dynamic interactive debugger called tf_debug. You run it from the command line\n",
    "\n",
    "- tfdbg is an interactive debugger that you can run from a terminal and attach to a local or remote TensorFlow session.\n",
    "\n",
    "- TensorBoard is a visual monitoring tool. You can look at evaluation metrics, look for over-fitting, layers that are dead, etc. Higher level debugging of neural networks, in other words\n",
    "\n",
    "### The flow of debugging\n",
    "\n",
    "1. Read the call trace, \n",
    "2. Read the error message, find out where problem is. \n",
    "3. Having found out the problem, fix it, make sure that it works on your fake data\n",
    "4. You can try it back on your full large data set and hopefully everything works"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# build the DAG\n",
    "a = tf.constant([3,5,7])\n",
    "b = tf.constant([1,2,3])\n",
    "\n",
    "c = tf.add(a,b)\n",
    "\n",
    "# run it\n",
    "session = tf.Session()\n",
    "numpy_c = session.run(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2\n",
    "\n",
    "What you will learn\n",
    "\n",
    "- you will learn how to create production-ready machine learning models the easy way\n",
    "- train on large datasets that do not fit in memory\n",
    "- monitor your training metrics in Tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator API\n",
    "\n",
    "- Estimators automatically surface key metrics during training that you can visualize in Tensor board\n",
    "- Estimators are designed with a data set API that handles out of memory data sets\n",
    "- Estimators come with the necessary cluster execution code already built in\n",
    "- Do you need checkpoints to pause and resume your training? Estimators have them\n",
    "- Estimators wrap your model to make it ready for ML-Engine's hyper-parameter tuning, and maybe also push it to production behind ML-Engine's managed and autoscaled prediction service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Made Estimator\n",
    "\n",
    "- How can we pack our data into the single input vector that linear regressor expect? The answer is in various ways depending on what data we are packing, and so that is where the feature columns API comes in handy\n",
    "- There are many more feature column types to choose from: columns for continuous values, you want to bucketized, word embeddings, column crosses, and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo House Pricing Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "tf.disable_v2_behavior() # disable v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"outdir\", ignore_errors = True) # start fresh each time / ignore checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'outdir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x643f27c50>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from outdir/model.ckpt-2000\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into outdir/model.ckpt.\n",
      "INFO:tensorflow:loss = 562276.25, step = 2001\n",
      "INFO:tensorflow:global_step/sec: 206.79\n",
      "INFO:tensorflow:loss = 559969.0, step = 2101 (0.485 sec)\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 2187 vs previous value: 2187. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "INFO:tensorflow:global_step/sec: 406.934\n",
      "INFO:tensorflow:loss = 557722.9, step = 2201 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 582.93\n",
      "INFO:tensorflow:loss = 555533.8, step = 2301 (0.171 sec)\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 2302 vs previous value: 2302. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "INFO:tensorflow:global_step/sec: 603.778\n",
      "INFO:tensorflow:loss = 553398.1, step = 2401 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 613.832\n",
      "INFO:tensorflow:loss = 551312.4, step = 2501 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 471.885\n",
      "INFO:tensorflow:loss = 549273.8, step = 2601 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 599.833\n",
      "INFO:tensorflow:loss = 547279.44, step = 2701 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 609.801\n",
      "INFO:tensorflow:loss = 545327.06, step = 2801 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 602.311\n",
      "INFO:tensorflow:loss = 543414.4, step = 2901 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 613.042\n",
      "INFO:tensorflow:loss = 541539.4, step = 3001 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 613.207\n",
      "INFO:tensorflow:loss = 539700.06, step = 3101 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 594.177\n",
      "INFO:tensorflow:loss = 537895.0, step = 3201 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 602.526\n",
      "INFO:tensorflow:loss = 536122.3, step = 3301 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 638.243\n",
      "INFO:tensorflow:loss = 534380.7, step = 3401 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 631.058\n",
      "INFO:tensorflow:loss = 532668.75, step = 3501 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 636.088\n",
      "INFO:tensorflow:loss = 530985.1, step = 3601 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 602.377\n",
      "INFO:tensorflow:loss = 529328.8, step = 3701 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 601.247\n",
      "INFO:tensorflow:loss = 527698.5, step = 3801 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 607.091\n",
      "INFO:tensorflow:loss = 526093.44, step = 3901 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 609.078\n",
      "INFO:tensorflow:loss = 524512.4, step = 4001 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 607.773\n",
      "INFO:tensorflow:loss = 522954.62, step = 4101 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 612.444\n",
      "INFO:tensorflow:loss = 521419.28, step = 4201 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 456.011\n",
      "INFO:tensorflow:loss = 519905.44, step = 4301 (0.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 497.151\n",
      "INFO:tensorflow:loss = 518412.47, step = 4401 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 605.272\n",
      "INFO:tensorflow:loss = 516939.72, step = 4501 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 584.843\n",
      "INFO:tensorflow:loss = 515486.3, step = 4601 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 527.018\n",
      "INFO:tensorflow:loss = 514051.8, step = 4701 (0.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 446.263\n",
      "INFO:tensorflow:loss = 512635.44, step = 4801 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 600.517\n",
      "INFO:tensorflow:loss = 511236.75, step = 4901 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 624.977\n",
      "INFO:tensorflow:loss = 509855.28, step = 5001 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 598.225\n",
      "INFO:tensorflow:loss = 508490.4, step = 5101 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 426.693\n",
      "INFO:tensorflow:loss = 507141.56, step = 5201 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 598.161\n",
      "INFO:tensorflow:loss = 505808.44, step = 5301 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 611.411\n",
      "INFO:tensorflow:loss = 504490.5, step = 5401 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 606.263\n",
      "INFO:tensorflow:loss = 503187.3, step = 5501 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 613.515\n",
      "INFO:tensorflow:loss = 501898.62, step = 5601 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 596.741\n",
      "INFO:tensorflow:loss = 500623.75, step = 5701 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 607.309\n",
      "INFO:tensorflow:loss = 499362.75, step = 5801 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 565.697\n",
      "INFO:tensorflow:loss = 498114.88, step = 5901 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 605.107\n",
      "INFO:tensorflow:loss = 496879.94, step = 6001 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 602.769\n",
      "INFO:tensorflow:loss = 495657.7, step = 6101 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 622.975\n",
      "INFO:tensorflow:loss = 494447.7, step = 6201 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 624.559\n",
      "INFO:tensorflow:loss = 493249.84, step = 6301 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 624.072\n",
      "INFO:tensorflow:loss = 492063.5, step = 6401 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 623.842\n",
      "INFO:tensorflow:loss = 490888.7, step = 6501 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 561.151\n",
      "INFO:tensorflow:loss = 489725.06, step = 6601 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.87\n",
      "INFO:tensorflow:loss = 488572.5, step = 6701 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 593.81\n",
      "INFO:tensorflow:loss = 487430.5, step = 6801 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 614.983\n",
      "INFO:tensorflow:loss = 486298.88, step = 6901 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 533.078\n",
      "INFO:tensorflow:loss = 485177.5, step = 7001 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 475.332\n",
      "INFO:tensorflow:loss = 484066.25, step = 7101 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 553.949\n",
      "INFO:tensorflow:loss = 482964.7, step = 7201 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 559.87\n",
      "INFO:tensorflow:loss = 481872.75, step = 7301 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 562.072\n",
      "INFO:tensorflow:loss = 480790.1, step = 7401 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 572.135\n",
      "INFO:tensorflow:loss = 479716.8, step = 7501 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 603.533\n",
      "INFO:tensorflow:loss = 478652.38, step = 7601 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 569.406\n",
      "INFO:tensorflow:loss = 477596.84, step = 7701 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 614.489\n",
      "INFO:tensorflow:loss = 476549.94, step = 7801 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 542.408\n",
      "INFO:tensorflow:loss = 475511.56, step = 7901 (0.185 sec)\n",
      "INFO:tensorflow:global_step/sec: 620.377\n",
      "INFO:tensorflow:loss = 474481.56, step = 8001 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 559.914\n",
      "INFO:tensorflow:loss = 473459.75, step = 8101 (0.179 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 540.392\n",
      "INFO:tensorflow:loss = 472445.8, step = 8201 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 546.953\n",
      "INFO:tensorflow:loss = 471439.88, step = 8301 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 566.803\n",
      "INFO:tensorflow:loss = 470441.75, step = 8401 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 660.393\n",
      "INFO:tensorflow:loss = 469451.06, step = 8501 (0.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 537.978\n",
      "INFO:tensorflow:loss = 468467.88, step = 8601 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 618.851\n",
      "INFO:tensorflow:loss = 467491.94, step = 8701 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 613.049\n",
      "INFO:tensorflow:loss = 466523.25, step = 8801 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 558.603\n",
      "INFO:tensorflow:loss = 465561.75, step = 8901 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 607.471\n",
      "INFO:tensorflow:loss = 464607.16, step = 9001 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 620.747\n",
      "INFO:tensorflow:loss = 463659.25, step = 9101 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 620.613\n",
      "INFO:tensorflow:loss = 462718.25, step = 9201 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 624.606\n",
      "INFO:tensorflow:loss = 461783.8, step = 9301 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 622.994\n",
      "INFO:tensorflow:loss = 460855.8, step = 9401 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 620.178\n",
      "INFO:tensorflow:loss = 459934.3, step = 9501 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 540.482\n",
      "INFO:tensorflow:loss = 459019.1, step = 9601 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 513.261\n",
      "INFO:tensorflow:loss = 458110.1, step = 9701 (0.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 541.102\n",
      "INFO:tensorflow:loss = 457207.12, step = 9801 (0.185 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.748\n",
      "INFO:tensorflow:loss = 456310.22, step = 9901 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 419.215\n",
      "INFO:tensorflow:loss = 455419.25, step = 10001 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 596.128\n",
      "INFO:tensorflow:loss = 454534.12, step = 10101 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 601.431\n",
      "INFO:tensorflow:loss = 453654.8, step = 10201 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 623.924\n",
      "INFO:tensorflow:loss = 452780.9, step = 10301 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 614.795\n",
      "INFO:tensorflow:loss = 451912.84, step = 10401 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 630.605\n",
      "INFO:tensorflow:loss = 451050.12, step = 10501 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 629.089\n",
      "INFO:tensorflow:loss = 450192.8, step = 10601 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 514.263\n",
      "INFO:tensorflow:loss = 449341.0, step = 10701 (0.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 635.918\n",
      "INFO:tensorflow:loss = 448494.25, step = 10801 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 552.435\n",
      "INFO:tensorflow:loss = 447652.7, step = 10901 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 617.726\n",
      "INFO:tensorflow:loss = 446816.38, step = 11001 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 639.141\n",
      "INFO:tensorflow:loss = 445985.0, step = 11101 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 629.497\n",
      "INFO:tensorflow:loss = 445158.6, step = 11201 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 613.354\n",
      "INFO:tensorflow:loss = 444337.2, step = 11301 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 627.148\n",
      "INFO:tensorflow:loss = 443520.62, step = 11401 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 620.903\n",
      "INFO:tensorflow:loss = 442708.75, step = 11501 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 615.755\n",
      "INFO:tensorflow:loss = 441901.75, step = 11601 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 626.151\n",
      "INFO:tensorflow:loss = 441099.28, step = 11701 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 632.343\n",
      "INFO:tensorflow:loss = 440301.4, step = 11801 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 577.204\n",
      "INFO:tensorflow:loss = 439508.2, step = 11901 (0.173 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12000 into outdir/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 438727.2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.linear.LinearRegressor at 0x643f27910>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_input_fn():\n",
    "    features = {\"sqr_foot\": [100, 2000, 3000, 400, 300, 9000],\n",
    "                \"type\": [\"apt\", \"house\", \"house\", \"house\", \"apt\", \"house\" ]}\n",
    "    \n",
    "    labels = [400, 700, 900, 300, 400, 1300]\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "featcols = [\n",
    "    tf.feature_column.numeric_column(\"sqr_foot\"),\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list(\"type\", [\"house\",\"apt\"])\n",
    "]\n",
    "\n",
    "model = tf.estimator.LinearRegressor(featcols, \"outdir\", \"./model_trained)\n",
    "\n",
    "model.train(train_input_fn, steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from outdir/model.ckpt-12000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "{'predictions': array([98.66314], dtype=float32)}\n",
      "{'predictions': array([239.65883], dtype=float32)}\n",
      "{'predictions': array([397.38232], dtype=float32)}\n",
      "{'predictions': array([129.25238], dtype=float32)}\n",
      "{'predictions': array([145.9802], dtype=float32)}\n",
      "{'predictions': array([1501.4469], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "def predict_input_fn():\n",
    "    features = {\"sqr_foot\": [100, 1000, 2000, 300, 400, 9000],\n",
    "                \"type\": [\"apt\", \"house\", \"house\", \"house\", \"apt\", \"house\" ]}\n",
    "    return features\n",
    "\n",
    "predictions = model.predict(predict_input_fn)\n",
    "\n",
    "print(next(predictions))\n",
    "print(next(predictions))\n",
    "print(next(predictions))\n",
    "print(next(predictions))\n",
    "print(next(predictions))\n",
    "print(next(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointing\n",
    "\n",
    "- Why are these important? They allow you to continue training, resume on failure, and predict from a train model. You get checkpoints for free, just specify a folder directory. And let's take a look at the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './model_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x6440cfd10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./model_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 3400000.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 212.627\n",
      "INFO:tensorflow:loss = 638049.56, step = 101 (0.472 sec)\n",
      "INFO:tensorflow:global_step/sec: 595.919\n",
      "INFO:tensorflow:loss = 628792.0, step = 201 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 587.869\n",
      "INFO:tensorflow:loss = 621681.8, step = 301 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 616.485\n",
      "INFO:tensorflow:loss = 615708.5, step = 401 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 622.661\n",
      "INFO:tensorflow:loss = 610469.0, step = 501 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 620.617\n",
      "INFO:tensorflow:loss = 605753.9, step = 601 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 610.366\n",
      "INFO:tensorflow:loss = 601437.44, step = 701 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 623.636\n",
      "INFO:tensorflow:loss = 597437.5, step = 801 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 616.412\n",
      "INFO:tensorflow:loss = 593696.9, step = 901 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 601.486\n",
      "INFO:tensorflow:loss = 590173.56, step = 1001 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 611.834\n",
      "INFO:tensorflow:loss = 586835.94, step = 1101 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 598.122\n",
      "INFO:tensorflow:loss = 583659.1, step = 1201 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 629.825\n",
      "INFO:tensorflow:loss = 580623.75, step = 1301 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 615.37\n",
      "INFO:tensorflow:loss = 577713.6, step = 1401 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 608.968\n",
      "INFO:tensorflow:loss = 574915.8, step = 1501 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 599.421\n",
      "INFO:tensorflow:loss = 572219.0, step = 1601 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 602.322\n",
      "INFO:tensorflow:loss = 569614.1, step = 1701 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 603.865\n",
      "INFO:tensorflow:loss = 567093.25, step = 1801 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 628.592\n",
      "INFO:tensorflow:loss = 564649.25, step = 1901 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 620.937\n",
      "INFO:tensorflow:loss = 562276.25, step = 2001 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 619.816\n",
      "INFO:tensorflow:loss = 559969.0, step = 2101 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 610.695\n",
      "INFO:tensorflow:loss = 557722.9, step = 2201 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 617.238\n",
      "INFO:tensorflow:loss = 555533.8, step = 2301 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 612.003\n",
      "INFO:tensorflow:loss = 553398.1, step = 2401 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 572.304\n",
      "INFO:tensorflow:loss = 551312.4, step = 2501 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 559.726\n",
      "INFO:tensorflow:loss = 549273.8, step = 2601 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 447.808\n",
      "INFO:tensorflow:loss = 547279.44, step = 2701 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 474.273\n",
      "INFO:tensorflow:loss = 545327.06, step = 2801 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 529.268\n",
      "INFO:tensorflow:loss = 543414.4, step = 2901 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 525.563\n",
      "INFO:tensorflow:loss = 541539.4, step = 3001 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.999\n",
      "INFO:tensorflow:loss = 539700.06, step = 3101 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 324.502\n",
      "INFO:tensorflow:loss = 537895.0, step = 3201 (0.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 513.724\n",
      "INFO:tensorflow:loss = 536122.3, step = 3301 (0.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 466.286\n",
      "INFO:tensorflow:loss = 534380.7, step = 3401 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 545.381\n",
      "INFO:tensorflow:loss = 532668.75, step = 3501 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 385.877\n",
      "INFO:tensorflow:loss = 530985.1, step = 3601 (0.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 680.378\n",
      "INFO:tensorflow:loss = 529328.8, step = 3701 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 685.801\n",
      "INFO:tensorflow:loss = 527698.5, step = 3801 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 619.349\n",
      "INFO:tensorflow:loss = 526093.44, step = 3901 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 643.331\n",
      "INFO:tensorflow:loss = 524512.4, step = 4001 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.148\n",
      "INFO:tensorflow:loss = 522954.62, step = 4101 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 519.918\n",
      "INFO:tensorflow:loss = 521419.28, step = 4201 (0.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 528.402\n",
      "INFO:tensorflow:loss = 519905.44, step = 4301 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 534.574\n",
      "INFO:tensorflow:loss = 518412.47, step = 4401 (0.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 455.803\n",
      "INFO:tensorflow:loss = 516939.72, step = 4501 (0.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 585.056\n",
      "INFO:tensorflow:loss = 515486.3, step = 4601 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 446.989\n",
      "INFO:tensorflow:loss = 514051.8, step = 4701 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 434.15\n",
      "INFO:tensorflow:loss = 512635.44, step = 4801 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 456.892\n",
      "INFO:tensorflow:loss = 511236.75, step = 4901 (0.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 493.44\n",
      "INFO:tensorflow:loss = 509855.28, step = 5001 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 488.255\n",
      "INFO:tensorflow:loss = 508490.4, step = 5101 (0.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 533.106\n",
      "INFO:tensorflow:loss = 507141.56, step = 5201 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 507.347\n",
      "INFO:tensorflow:loss = 505808.44, step = 5301 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 422.429\n",
      "INFO:tensorflow:loss = 504490.5, step = 5401 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 589.528\n",
      "INFO:tensorflow:loss = 503187.3, step = 5501 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 612.223\n",
      "INFO:tensorflow:loss = 501898.62, step = 5601 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 633.963\n",
      "INFO:tensorflow:loss = 500623.75, step = 5701 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 629.568\n",
      "INFO:tensorflow:loss = 499362.75, step = 5801 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 472.57\n",
      "INFO:tensorflow:loss = 498114.88, step = 5901 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.407\n",
      "INFO:tensorflow:loss = 496879.94, step = 6001 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 580.625\n",
      "INFO:tensorflow:loss = 495657.7, step = 6101 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 450.049\n",
      "INFO:tensorflow:loss = 494447.7, step = 6201 (0.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 470.487\n",
      "INFO:tensorflow:loss = 493249.84, step = 6301 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 577.407\n",
      "INFO:tensorflow:loss = 492063.5, step = 6401 (0.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 598.831\n",
      "INFO:tensorflow:loss = 490888.7, step = 6501 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 559.162\n",
      "INFO:tensorflow:loss = 489725.06, step = 6601 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 532.153\n",
      "INFO:tensorflow:loss = 488572.5, step = 6701 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 514.668\n",
      "INFO:tensorflow:loss = 487430.5, step = 6801 (0.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 470.23\n",
      "INFO:tensorflow:loss = 486298.88, step = 6901 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 548.896\n",
      "INFO:tensorflow:loss = 485177.5, step = 7001 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 530.882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 484066.25, step = 7101 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 571.963\n",
      "INFO:tensorflow:loss = 482964.7, step = 7201 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 609.489\n",
      "INFO:tensorflow:loss = 481872.75, step = 7301 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 497.062\n",
      "INFO:tensorflow:loss = 480790.1, step = 7401 (0.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 498.331\n",
      "INFO:tensorflow:loss = 479716.8, step = 7501 (0.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 564.668\n",
      "INFO:tensorflow:loss = 478652.38, step = 7601 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 573.74\n",
      "INFO:tensorflow:loss = 477596.84, step = 7701 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 582.364\n",
      "INFO:tensorflow:loss = 476549.94, step = 7801 (0.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 638.507\n",
      "INFO:tensorflow:loss = 475511.56, step = 7901 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 609.418\n",
      "INFO:tensorflow:loss = 474481.56, step = 8001 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 464.233\n",
      "INFO:tensorflow:loss = 473459.75, step = 8101 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 507.115\n",
      "INFO:tensorflow:loss = 472445.8, step = 8201 (0.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 536.126\n",
      "INFO:tensorflow:loss = 471439.88, step = 8301 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 589.202\n",
      "INFO:tensorflow:loss = 470441.75, step = 8401 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 631.661\n",
      "INFO:tensorflow:loss = 469451.06, step = 8501 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 607.711\n",
      "INFO:tensorflow:loss = 468467.88, step = 8601 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 647.639\n",
      "INFO:tensorflow:loss = 467491.94, step = 8701 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 575.473\n",
      "INFO:tensorflow:loss = 466523.25, step = 8801 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 471.44\n",
      "INFO:tensorflow:loss = 465561.75, step = 8901 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 451.054\n",
      "INFO:tensorflow:loss = 464607.16, step = 9001 (0.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 570.223\n",
      "INFO:tensorflow:loss = 463659.25, step = 9101 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 501.565\n",
      "INFO:tensorflow:loss = 462718.25, step = 9201 (0.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 630.278\n",
      "INFO:tensorflow:loss = 461783.8, step = 9301 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 519.213\n",
      "INFO:tensorflow:loss = 460855.8, step = 9401 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 645.574\n",
      "INFO:tensorflow:loss = 459934.3, step = 9501 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 460.611\n",
      "INFO:tensorflow:loss = 459019.1, step = 9601 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 635.486\n",
      "INFO:tensorflow:loss = 458110.1, step = 9701 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 632.463\n",
      "INFO:tensorflow:loss = 457207.12, step = 9801 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 645.807\n",
      "INFO:tensorflow:loss = 456310.22, step = 9901 (0.155 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into ./model_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 455428.2.\n",
      "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./model_trained/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "{'predictions': array([91.748405], dtype=float32)}\n",
      "{'predictions': array([233.70328], dtype=float32)}\n",
      "{'predictions': array([392.5359], dtype=float32)}\n",
      "{'predictions': array([122.520424], dtype=float32)}\n",
      "{'predictions': array([139.3982], dtype=float32)}\n",
      "{'predictions': array([1504.3643], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "## Create Checkpoint\n",
    "# The model will continue from the checkpoint\n",
    "\n",
    "model = tf.estimator.LinearRegressor(featcols, \"./model_trained\") # directory\n",
    "\n",
    "model.train(train_input_fn, steps=10000)\n",
    "\n",
    "predictions = model.predict(predict_input_fn)\n",
    "\n",
    "print(next(predictions))\n",
    "print(next(predictions))\n",
    "print(next(predictions))\n",
    "print(next(predictions))\n",
    "print(next(predictions))\n",
    "print(next(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on in-memory datasets\n",
    "\n",
    "- If your data fits a memory in the form of either numpy arrays or Pandas, the Estimator API has easy convenience functions for feeding them into your model\n",
    "- Typically, training works best when one training step is performed on what is called a mini batch of input data at a time, not a single data item and not the entire data set either"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Tensor Flow using Batching\n",
    "\n",
    "- TensorFlow has an API called datasets that can handle this and feed your model while loading the data from disk in a progressive way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big jobs, Distributed training\n",
    "\n",
    "- You have to distribute it on a cluster to make it faster. The function that implements distributed training is called estimator.train and evaluate\n",
    "\n",
    "- With the estimator API and ML engine managing the cluster automatically, you get distribution out of the box\n",
    "\n",
    "- The traditional distribution model for training neural networks is called data parallelism. Your model is replicated on multiple workers\n",
    "\n",
    "- In distributed training even with a well-shuffled data set on disk, if all your workers are loading straight from this data set, they will be seeing the same batch of data, at the same time, and produce the same gradients.\n",
    "\n",
    "- With data set that shuffle, the shuffling happens independently on each worker using a different random seed, so please use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring with Tensorboards\n",
    "\n",
    "- TensorBoard is a tool that lets you visualize the training and the biometrics that your model writes to disk\n",
    "\n",
    "- Ready for deployment with not only a checkpoint on good trained parameters, but also an extra input function that will map between the JSON received by the REST API and the features as expected by the model. This one is called the serving input function\n",
    "\n",
    "- Serving and training time inputs are often very different\n",
    "\n",
    "- At training time this is done through the training input function. We use the data as an API there to make an input node that could progressively read from CSV files and send batches of training data into the model.\n",
    "\n",
    "- We will use a similar pattern for our deployed model. The serving input function lets us add a set of TensorFlow transformations between the JSON our REST API receives and the features expected by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
